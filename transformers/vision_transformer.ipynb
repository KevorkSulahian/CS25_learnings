{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Split image into patches and them embed them\n",
    "\n",
    "    Parameters \n",
    "    ----------\n",
    "\n",
    "    img_size: int\n",
    "        Size of image (square assumed)\n",
    "\n",
    "    patch_size: int\n",
    "        Size of each patch (square)\n",
    "\n",
    "    in_chans: int\n",
    "        Number of input channels\n",
    "\n",
    "    embed_dim: int\n",
    "        The embedding dimension\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    n_patches: int\n",
    "        Number of patches inside of our image\n",
    "\n",
    "    proj: nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches and their embedding\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Run forward pass\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            Shape `(n_samples, in_chans, img_size, img_size)`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches, embed_dim)`\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.proj(x) # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        \n",
    "        x = x.flatten(2) # (n_samples, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2) # (n_samples, n_patches, embed_dim)\n",
    "        return x\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim: int\n",
    "        The input and out dimension of per token features\n",
    "    \n",
    "    n_heads: int\n",
    "        Number of attention heads\n",
    "\n",
    "    qkv_bias: bool\n",
    "        If True then we include bias to the query, key and value projections\n",
    "\n",
    "    attn_p: float\n",
    "        Dropout probability applied to the query, key and value tensors\n",
    "\n",
    "    proj_p: float\n",
    "        Dropout probability applied to the output tensor\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scale: float\n",
    "        Normalizing consant for the dot product\n",
    "\n",
    "    qkv: nn.Linear\n",
    "\n",
    "    proj: nn.Linear\n",
    "        Linear mapping that takes in the concatenated output of all attention heads and maps it into a new space\n",
    "\n",
    "    attn_drop, proj_drop: nn.Dropout\n",
    "        Dropout layers\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        head_dim = dim // n_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        x: torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`. The +1 is for the [CLS] token\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`\n",
    "\n",
    "        \"\"\"\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        \n",
    "        if dim != self.n_heads:\n",
    "            raise ValueError\n",
    "        \n",
    "        qkv = self.qkv(x) # (n_samples, n_patches + 1, 3 * dim)\n",
    "        qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads).permute(2, 0, 3, 1)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # (n_samples, n_heads, n_patches + 1, head_dim)\n",
    "        k_t = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_patches + 1)\n",
    "        dp = (q @ k_t) * self.scale\n",
    "        attn = dp.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        weighted_avg = attn @ v # (n_samples, n_heads, n_patches + 1, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "\n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "class MLP(nn.Module):   \n",
    "    \"\"\"\n",
    "    Multilayer perceptron\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features: int\n",
    "        Number of input features\n",
    "\n",
    "    hidden_features: int\n",
    "        Number of nodes in the hidden layer\n",
    "\n",
    "    out_features: int\n",
    "        Number of output features\n",
    "\n",
    "    p: float\n",
    "        Dropout probability\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    fc: nn.Linear\n",
    "        The first linear layer\n",
    "\n",
    "    act: nn.GELU\n",
    "        GELU activation function\n",
    "\n",
    "    fc2: nn.Linear\n",
    "        The second linear layer\n",
    "\n",
    "    drop: nn.Dropout\n",
    "        Dropout layer\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, p=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        x: torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`. The +1 is for the [CLS] token\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim: int\n",
    "        Embedding dimension\n",
    "\n",
    "    n_heads: int   \n",
    "        Number of attention heads\n",
    "\n",
    "    mlp_ratio: float\n",
    "        Determines the hidden dimension size of the `MLP` module relative to `dim`\n",
    "\n",
    "    qkv_bias: bool\n",
    "        If True then we include bias to the query, key and value projections\n",
    "\n",
    "    p, attn_p, proj_p: float\n",
    "        Dropout probabilities\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2: nn.LayerNorm\n",
    "        Layer normalization\n",
    "\n",
    "    attn: Attention\n",
    "        Attention module\n",
    "\n",
    "    mlp: MLP\n",
    "        MLP module\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4., qkv_bias=True, p=0., attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(\n",
    "            dim, n_heads=n_heads, qkv_bias=qkv_bias, attn_p=attn_p, proj_p=proj_p\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=hidden_dim, out_features=dim, p=p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        x: torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`. The +1 is for the [CLS] token\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`\n",
    "\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size: int\n",
    "        Both height and the width of the image\n",
    "\n",
    "    patch_size: int\n",
    "        Both height and the width of the patch\n",
    "\n",
    "    in_chans: int\n",
    "        Number of input channels\n",
    "\n",
    "    n_classes: int\n",
    "        Number of classes of the classification problem\n",
    "\n",
    "    embed_dim: int\n",
    "        Dimensionality of the token/patch embeddings\n",
    "\n",
    "    depth: int\n",
    "        Number of blocks in the `Block` class\n",
    "\n",
    "    n_heads: int\n",
    "        Number of attention heads inside of the `Attention` class\n",
    "\n",
    "    mlp_ratio: float\n",
    "        Determines the hidden dimension size of the `MLP` module relative to `embed_dim`\n",
    "\n",
    "    qkv_bias: bool\n",
    "        If True then we include bias to the query, key and value projections\n",
    "\n",
    "    p, attn_p, proj_p: float\n",
    "        Dropout probabilities\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed: PatchEmbed\n",
    "        Instance of `PatchEmbed` layer\n",
    "\n",
    "    cls_token: nn.Parameter\n",
    "        Learnable parameter that will represent the first token in the sequence.\n",
    "        It has `embed_dim` elements\n",
    "\n",
    "    pos_emb: nn.Parameter\n",
    "        Positional embedding of the cls_token + all the patches.\n",
    "        It has `(n_patches + 1) * embed_dim` elements\n",
    "\n",
    "    pos_drop: nn.Dropout\n",
    "        Dropout layer\n",
    "\n",
    "    blocks: nn.ModuleList\n",
    "        List of `Block` layers\n",
    "\n",
    "    norm: nn.LayerNorm\n",
    "        Layer normalization\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=384,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        n_classes=1000,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        n_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        p=0.,\n",
    "        attn_p=0.,\n",
    "        proj_p=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim=embed_dim, n_heads=n_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, p=p, attn_p=attn_p, proj_p=proj_p)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        x: torch.Tensor\n",
    "            Shape `(n_samples, in_chans, img_size, img_size)`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_classes)`\n",
    "\n",
    "        \"\"\"\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(n_samples, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.pos_emb\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timm\n",
    "\n",
    "# helpers\n",
    "def get_n_params(module):\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "def assert_tensors_equal(t1, t2):\n",
    "    a1, a2 = t1.detach().numpy(), t2.detach().numpy()\n",
    "\n",
    "    np.testing.assert_allclose(a1, a2)\n",
    "\n",
    "model_name = 'vit_base_patch16_384'\n",
    "model_official = timm.create_model(model_name, pretrained=True)\n",
    "model_official.eval()\n",
    "print(type(model_official))\n",
    "\n",
    "custom_config = {\n",
    "    'img_size': 384,\n",
    "    'patch_size': 16,\n",
    "    'in_chans': 3,\n",
    "    'n_classes': 1000,\n",
    "    'embed_dim': 768,\n",
    "    'depth': 12,\n",
    "    'n_heads': 12,\n",
    "    'mlp_ratio': 4\n",
    "}\n",
    "\n",
    "model_custom = VisionTransformer(**custom_config)\n",
    "model_custom.eval()\n",
    "\n",
    "for (n_o, p_o), (n_c, p_c) in zip(model_official.named_parameters(), model_custom.named_parameters()):\n",
    "    assert n_o == n_c\n",
    "    assert p_o.shape == p_c.shape\n",
    "    p_c.data = p_o.data\n",
    "    assert_tensors_equal(p_o, p_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
